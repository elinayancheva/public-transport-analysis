{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Public transport analysis \n",
    "Data Science project by Elina Yancheva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/transport_data.csv')\n",
    "display(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invalid entries search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "invalid_data = {\n",
    "    # Bounds of Sofia region (latitude: 42-43, longitude: 23-24)\n",
    "    'tap_lat': sum((df['tap_lat'] < 42) | (df['tap_lat'] > 43) | df['tap_lat'].isna()),\n",
    "    'tap_lon': sum((df['tap_lon'] < 23) | (df['tap_lon'] > 24) | df['tap_lon'].isna()),\n",
    "    'origin_stop_lat': sum((df['origin_stop_lat'] < 42) | (df['origin_stop_lat'] > 43) | df['origin_stop_lat'].isna()),\n",
    "    'origin_stop_lon': sum((df['origin_stop_lon'] < 23) | (df['origin_stop_lon'] > 24) | df['origin_stop_lon'].isna()),\n",
    "    'dest_stop_lat': sum((df['dest_stop_lat'] < 42) | (df['dest_stop_lat'] > 43) | df['dest_stop_lat'].isna()),\n",
    "    'dest_stop_lon': sum((df['dest_stop_lon'] < 23) | (df['dest_stop_lon'] > 24) | df['dest_stop_lon'].isna()),\n",
    "    'boarding_t': sum(df['boarding_t'] < 0),\n",
    "    'transfer_t': sum(df['transfer_t'] < 0),\n",
    "    'boarding_dist': sum(df['boarding_dist'] < 0),\n",
    "    'transfer_dist': sum(df['transfer_dist'] < 0),\n",
    "}\n",
    "\n",
    "# percentage of invalid data\n",
    "total_cells = df.shape[0] * df.shape[1]\n",
    "total_invalid = sum(invalid_data.values())\n",
    "invalid_percentage = (total_invalid / total_cells) * 100\n",
    "\n",
    "print(\"\\nInvalid data counts by specific validation rules:\")\n",
    "for col, count in invalid_data.items():\n",
    "    if count > 0:\n",
    "        print(f\"{col}: {count} invalid values ({(count / len(df)) * 100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nTotal cells in dataset: {total_cells}\")\n",
    "print(f\"Total invalid cells: {total_invalid}\")\n",
    "print(f\"Percentage of invalid data: {invalid_percentage:.2f}%\")\n",
    "\n",
    "# origin timestamp should be before destination timestamp\n",
    "time_inconsistency = sum(df['origin_ts'] >= df['dest_ts'])\n",
    "print(f\"\\nRows with origin timestamp >= destination timestamp: {time_inconsistency} ({(time_inconsistency / len(df)) * 100:.2f}%)\")\n",
    "\n",
    "# duplicate IDs\n",
    "duplicate_ids = df['id'].duplicated().sum()\n",
    "print(f\"Duplicate IDs: {duplicate_ids} ({(duplicate_ids / len(df)) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative values search\n",
    " Check for negative values in numeric columns where negative values would be problematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "negative_value_checks = {}\n",
    "\n",
    "# Time-related variables shouldn't be negative\n",
    "for col in ['boarding_t', 'transfer_t', 'origin_ts', 'dest_ts']:\n",
    "    if col in df.columns:\n",
    "        neg_count = (df[col] < 0).sum()\n",
    "        neg_pct = (df[col] < 0).mean() * 100 if neg_count > 0 else 0\n",
    "        negative_value_checks[col] = {'count': neg_count, 'percentage': neg_pct}\n",
    "\n",
    "# Distance-related variables shouldn't be negative\n",
    "for col in ['boarding_dist', 'transfer_dist']:\n",
    "    if col in df.columns:\n",
    "        neg_count = (df[col] < 0).sum()\n",
    "        neg_pct = (df[col] < 0).mean() * 100 if neg_count > 0 else 0\n",
    "        negative_value_checks[col] = {'count': neg_count, 'percentage': neg_pct}\n",
    "\n",
    "\n",
    "print(\"Negative Value Checks:\")\n",
    "for col, stats in negative_value_checks.items():\n",
    "    if stats['count'] > 0:\n",
    "        print(f\"{col}: {stats['count']} negative values ({stats['percentage']:.2f}%)\")\n",
    "\n",
    "if 'transfer_t' in df.columns and (df['transfer_t'] < -600).sum() > 0:\n",
    "    print(f\"\\nExtreme negative transfer times (< -10 min): {(df['transfer_t'] < -600).sum()} records\")\n",
    "    # examples of these problematic records\n",
    "    print(\"\\nSample of records with extremely negative transfer times:\")\n",
    "    print(f\"\\nInvalid transfer times by transport type:\\n {df[df['transfer_t'] <= 0]['transport_type'].value_counts()}\")\n",
    "\n",
    "\n",
    "# zero transfer time\n",
    "if 'transfer_t' in df.columns and 'is_transfer' in df.columns:\n",
    "    zero_transfer_time_count = ((df['is_transfer'] == True) & (df['transfer_t'] == 0)).sum()\n",
    "    if zero_transfer_time_count > 0:\n",
    "        print(f\"\\nTransfers with zero transfer time: {zero_transfer_time_count} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate IDs search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_ids = df['id'].value_counts()\n",
    "duplicate_ids = duplicate_ids[duplicate_ids > 1]\n",
    "\n",
    "print(f\"Number of unique IDs that appear multiple times: {len(duplicate_ids)}\")\n",
    "print(f\"Total number of rows with duplicate IDs: {sum(duplicate_ids) - len(duplicate_ids)}\")\n",
    "\n",
    "# For each duplicate ID, check if the rows are identical\n",
    "print(\"\\nAnalyzing duplicate ID rows:\")\n",
    "identical_dupes = 0\n",
    "different_dupes = 0\n",
    "dupe_analysis = {}\n",
    "\n",
    "for current_id in duplicate_ids.index:\n",
    "    id_rows = df[df['id'] == current_id]\n",
    "    \n",
    "    # First, reset the index to avoid comparing index values\n",
    "    id_rows_reset = id_rows.reset_index(drop=True)\n",
    "    \n",
    "    # Check if all rows are identical\n",
    "    all_identical = id_rows_reset.iloc[0:1].equals(id_rows_reset)\n",
    "    \n",
    "    # If not all identical, find which columns differ\n",
    "    if not all_identical:\n",
    "        different_dupes += 1\n",
    "    else:\n",
    "        identical_dupes += 1\n",
    "\n",
    "print(f\"\\nOut of {len(duplicate_ids)} IDs with duplicates:\")\n",
    "print(f\"- {identical_dupes} IDs have completely identical rows\")\n",
    "print(f\"- {different_dupes} IDs have differences between rows\")\n",
    "\n",
    "# are there multiple unique dates in the dataset\n",
    "unique_dates = df['date'].nunique()\n",
    "if unique_dates == 1:\n",
    "    print(f\"The dataset contains a single date: {df['date'].iloc[0]}\")\n",
    "else:\n",
    "    # check if duplicates occur within the same day\n",
    "    if 'date' in df.columns:\n",
    "        same_day_dupes = 0\n",
    "        for current_id in duplicate_ids.index:\n",
    "            dates = df[df['id'] == current_id]['date'].unique()\n",
    "            if len(dates) == 1:\n",
    "                same_day_dupes += 1\n",
    "        \n",
    "        print(f\"\\nDuplicate IDs occurring on the same day: {same_day_dupes} ({same_day_dupes/len(duplicate_ids)*100:.1f}%)\")\n",
    "        print(f\"Duplicate IDs occurring across different days: {len(duplicate_ids) - same_day_dupes} ({(len(duplicate_ids) - same_day_dupes)/len(duplicate_ids)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analysis, we discovered that all duplicate IDs (13,230 unique IDs appearing 23,888 times) represent different transport journeys rather than identical data entries. Since these represent legitimate separate journeys maybe made by the same user/card, we will retain all rows in our analysis without filtering for unique IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_id_values = duplicate_ids.index.tolist()\n",
    "\n",
    "df_duplicates = df[df['id'].isin(duplicate_id_values)]\n",
    "\n",
    "transfer_counts = df_duplicates['is_transfer'].value_counts(normalize=True) * 100\n",
    "\n",
    "all_transfers = df_duplicates.groupby('id')['is_transfer'].all().mean() * 100\n",
    "any_transfers = df_duplicates.groupby('id')['is_transfer'].any().mean() * 100\n",
    "none_transfers = (~df_duplicates.groupby('id')['is_transfer'].any()).mean() * 100\n",
    "\n",
    "print(f\"Total rows with duplicate IDs: {len(df_duplicates)}\")\n",
    "print(f\"Percentage of duplicate ID rows where is_transfer = True: {transfer_counts.get(True, 0):.2f}%\")\n",
    "print(f\"Percentage of duplicate ID rows where is_transfer = False: {transfer_counts.get(False, 0):.2f}%\")\n",
    "\n",
    "# transfer patterns by journey sequence\n",
    "print(\"\\nAnalyzing transfer patterns by journey sequence:\")\n",
    "sequence_patterns = {}\n",
    "\n",
    "for id_val in duplicate_ids.index:\n",
    "    # rows for this ID, SORTED BY ORIGIN TIMESTAMP\n",
    "    id_rows = df[df['id'] == id_val].sort_values('origin_ts')\n",
    "    \n",
    "    # pattern of transfers (T = transfer, N = non-transfer)\n",
    "    pattern = ''.join(['T' if x else 'N' for x in id_rows['is_transfer']])\n",
    "    \n",
    "    if pattern in sequence_patterns:\n",
    "        sequence_patterns[pattern] += 1\n",
    "    else:\n",
    "        sequence_patterns[pattern] = 1\n",
    "\n",
    "print(\"\\nMost common transfer patterns (T=transfer, N=non-transfer):\")\n",
    "for pattern, count in sorted(sequence_patterns.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"{pattern}: {count} IDs ({count/len(duplicate_ids)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Pattern Analysis for Duplicate IDs\n",
    "\n",
    "Analysis of the 37,118 journeys with duplicate IDs reveals that 46.61% involve transfers, with the most common pattern (34.01%) being a transfer followed by a non-transfer journey (TN). This suggests that many users make connected trips where they transfer once and then complete their journey directly, while the presence of various multi-segment patterns (TNT, TNN, TNTN) indicates more complex travel behaviors spanning multiple transit modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting millisecond timestamps to datetime\n",
    "df['dest_datetime'] = pd.to_datetime(df['dest_ts'], unit='ms')\n",
    "df['hour_of_day'] = df['dest_datetime'].dt.hour if 'dest_datetime' in df.columns else None\n",
    "\n",
    "df['trip_duration_min'] = df['boarding_t'] / 60  # assuming boarding_t is in seconds\n",
    "\n",
    "for col in df.columns:\n",
    "    missing_pct = df[col].isnull().mean() * 100\n",
    "    if missing_pct > 0:\n",
    "        print(f\"{col}: {missing_pct:.2f}% missing\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head of lines with missing og_line_id\n",
    "missing_og_line = df[df['og_line_id'].isnull()]\n",
    "print(missing_og_line['transport_type'].value_counts())\n",
    "print(\"---\")\n",
    "print(f\"All metro records don't have og_line_id: \\\n",
    "       {df[df['transport_type'] == 'metro']['og_line_id'].isnull().sum() == df[df['transport_type'] == 'metro'].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each column with significant missing data, check transport type distribution\n",
    "for col in ['media_id', 'product_id', 'pan', 'og_line_id', 'task_id', 'boarding_dist']:\n",
    "    print(f\"\\nMissing {col} by transport type:\")\n",
    "    print(df[df[col].isnull()]['transport_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metro** has the most missing data across all fields, with complete absence of `product_id`, `og_line_id`, `task_id`, and `boarding_dist` values. This suggests metro journeys are tracked differently in the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Value counts for dest_datetime (date only):\")\n",
    "print(df['dest_datetime'].dt.date.value_counts())\n",
    "\n",
    "print(\"\\nValue counts for date:\")\n",
    "print(df['date'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Inconsistency Analysis\n",
    "\n",
    "A critical data issue has been identified - while all records show the journey date as `2024-03-01`, the destination timestamps overwhelmingly resolve to `2025-03-01`  with two outliers on `2025-03-02` (maybe short after midnight). This one-year discrepancy between journey dates and destination timestamps indicates a systematic timestamp error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transport type distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['transport_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 3 records (0.005% of dataset) with \"unknown\" transport type to ensure accurate transport-specific analysis.\n",
    "df = df[df['transport_type'] != 'unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all transport types to lower case to merge 'Tram' and 'tram' etc.\n",
    "df['transport_type'] = df['transport_type'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport_counts = df['transport_type'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6), facecolor='#1f2937')\n",
    "ax.set_facecolor('#1f2937')\n",
    "\n",
    "sns.barplot(x=transport_counts.index, y=transport_counts.values, ax=ax)\n",
    "\n",
    "ax.set_title('Distribution of Transport Types', color=\"white\")\n",
    "ax.set_xlabel('Transport Type', color=\"white\")\n",
    "ax.set_ylabel('Number of Trips', color=\"white\")\n",
    "ax.tick_params(colors='white')\n",
    "\n",
    "# Add bar values (with white text)\n",
    "for i, v in enumerate(transport_counts.values):\n",
    "    ax.text(i, v + 5, str(v), ha='center', va='bottom', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trip timing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['origin_datetime'] = pd.to_datetime(df['origin_ts'], unit='ms')\n",
    "# extract hour from timestamp if not already done\n",
    "if 'hour_of_day' not in df.columns:\n",
    "    df['hour_of_day'] = df['origin_datetime'].dt.hour\n",
    "\n",
    "hourly_by_transport = df.groupby(['hour_of_day', 'transport_type']).size().unstack(fill_value=0)\n",
    "\n",
    "# Bar chart of total journeys by hour\n",
    "hourly_total = df.groupby('hour_of_day').size()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = hourly_total.plot(kind='bar', color='skyblue')\n",
    "plt.title('Total Journeys by Hour of Day', fontsize=14)\n",
    "plt.xlabel('Hour of Day', fontsize=12)\n",
    "plt.ylabel('Number of Journeys', fontsize=12)\n",
    "plt.grid(True, axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "for i, v in enumerate(hourly_total):\n",
    "    ax.text(i, v + 0.1, str(v), ha='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if 'hour_of_day' in df.columns and df['hour_of_day'] is not None:\n",
    "    hourly_trips = df.groupby(['hour_of_day', 'transport_type']).size().unstack().fillna(0)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    hourly_trips.plot(kind='line', marker='o')\n",
    "    plt.title('Hourly Trip Distribution by Transport Type')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Number of Trips')\n",
    "    plt.xticks(range(0, 24))\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transport Usage Analysis Summary\n",
    "\n",
    "**Transport Type Distribution:**\n",
    "- Metro dominates with ~37,000 trips (70%+), followed by bus (~5,800 trips)\n",
    "\n",
    "**Hourly Usage Patterns:**\n",
    "- Clear morning peak (8-10) and evening peak (16-19) for all transport modes\n",
    "- Metro volumes significantly exceed all other transportation types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_stats = df.groupby('transport_type')['is_transfer'].mean() * 100\n",
    "plt.figure(figsize=(10, 6))\n",
    "transfer_stats.plot(kind='bar', color='orange')\n",
    "plt.title('Percentage of Transfers by Transport Type')\n",
    "plt.ylabel('Transfer Percentage (%)')\n",
    "plt.xlabel('Transport Type')\n",
    "plt.axhline(y=df['is_transfer'].mean()*100, color='red', linestyle='--', alpha=0.7, \n",
    "            label=f'Overall Average: {df[\"is_transfer\"].mean()*100:.1f}%')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# transfer time analysis\n",
    "if 'transfer_t' in df.columns:\n",
    "    # Filter for trips with transfers and avoid invalid transfer times\n",
    "    transfer_trips = df[df['is_transfer'] == True & (df['transfer_t'] > 0)]\n",
    "    \n",
    "    # transfer time by transport type\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='transport_type', y='transfer_t', data=transfer_trips)\n",
    "    plt.title('Transfer Time by Transport Type')\n",
    "    plt.ylabel('Transfer Time (seconds)')\n",
    "    plt.xlabel('Transport Type')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # efficiency metrics\n",
    "    avg_transfer_time = df[df['is_transfer']]['transfer_t'].mean() / 60  # minutes\n",
    "    print(f\"Average transfer waiting time: {avg_transfer_time:.2f} minutes\")\n",
    "\n",
    "# transfer disance analysis\n",
    "if 'transfer_dist' in df.columns:\n",
    "    transfer_trips = df[df['is_transfer'] == True]\n",
    "    \n",
    "    # transfer distance by transport type\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='transport_type', y='transfer_dist', data=transfer_trips)\n",
    "    plt.title('Transfer Distance by Transport Type')\n",
    "    plt.ylabel('Transfer Distance (meters)')\n",
    "    plt.xlabel('Transport Type')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # calculate efficiency metrics\n",
    "    avg_transfer_distance = df[df['is_transfer']]['transfer_dist'].mean() / 1000  # kilometers\n",
    "    print(f\"Average transfer distance: {avg_transfer_distance:.2f} kilometers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Analysis Insights\n",
    "\n",
    "### Transfer Frequency\n",
    "- **Bus** has the highest transfer rate at ~44%, significantly above the overall average of 36.2%.\n",
    "- **Metro** has the lowest transfer rate at ~34%, suggesting it may serve more direct routes or complete journeys.\n",
    "- **Tram** and **Trolleybus** show transfer rates close to or slightly below the overall average.\n",
    "\n",
    "### Transfer Time\n",
    "- **Transfer times** typically range from ~150-750 seconds (2.5-12.5 minutes) across all transport types.\n",
    "- **Tram** shows the highest median transfer time, suggesting potentially less frequent service.\n",
    "- **Metro** displays notable outliers with some negative transfer times, indicating potential data quality issues or system timing anomalies.\n",
    "\n",
    "### Transfer Distance\n",
    "- **Trolleybus** transfers involve the longest distances (median ~100m).\n",
    "- **Metro** shows a weird distribution with many transfers happening at very short distances (0-1m) but also having numerous outliers stretching to 500m.\n",
    "- **Bus** and **Tram** show similar distance distributions with medians around 75-100m.\n",
    "\n",
    "These patterns suggest that while metro is the dominant transport mode, bus connections play a crucial role in the overall network connectivity. The presence of negative transfer times and the unusual metro distance distribution suggest further investigation into data quality and how transfers are recorded in the system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geospatial analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a map of all trip origins\n",
    "m = folium.Map(location=[df['tap_lat'].mean(), df['tap_lon'].mean()], zoom_start=12)\n",
    "\n",
    "# heatmap layer\n",
    "heat_data = [[row['tap_lat'], row['tap_lon']] for _, row in df.iterrows()]\n",
    "HeatMap(heat_data).add_to(m)\n",
    "m.save('trip_origin_heatmap.html')\n",
    "\n",
    "transport_colors = {\n",
    "    'bus': 'blue',\n",
    "    'metro': 'red',\n",
    "    'trolleybus': 'green',\n",
    "    'tram':  'orange'\n",
    "}\n",
    "\n",
    "m2 = folium.Map(location=[df['tap_lat'].mean(), df['tap_lon'].mean()], zoom_start=12)\n",
    "\n",
    "# add SMALL SAMPLE of points colored by transport type \n",
    "sample_size = min(1000, len(df))\n",
    "for _, row in df.sample(sample_size).iterrows():\n",
    "    color = transport_colors.get(row['transport_type'], 'gray')\n",
    "    folium.CircleMarker(\n",
    "        location=[row['tap_lat'], row['tap_lon']],\n",
    "        radius=3,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_opacity=0.7,\n",
    "        popup=row['transport_type']\n",
    "    ).add_to(m2)\n",
    "\n",
    "legend_html = '''\n",
    "<div style=\"position: fixed; bottom: 50px; left: 50px; z-index: 1000; background-color: white; \n",
    "padding: 10px; border: 2px solid grey; border-radius: 5px;\">\n",
    "<p><b>Transport Type</b></p>\n",
    "'''\n",
    "for transport, color in transport_colors.items():\n",
    "    legend_html += f'<p><i class=\"fa fa-circle\" style=\"color:{color}\"></i> {transport}</p>'\n",
    "legend_html += '</div>'\n",
    "\n",
    "m2.get_root().html.add_child(folium.Element(legend_html))\n",
    "m2.save('transport_type_map.html')\n",
    "\n",
    "display(m)\n",
    "display(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import HeatMapWithTime\n",
    "\n",
    "m4 = folium.Map(location=[df['origin_stop_lat'].mean(), df['origin_stop_lon'].mean()], zoom_start=12)\n",
    "\n",
    "# Create a column for hour if it doesn't exist yet\n",
    "if 'hour' not in df.columns and 'dest_datetime' in df.columns:\n",
    "    df['hour'] = df['dest_datetime'].dt.hour\n",
    "elif 'hour' not in df.columns and 'origin_ts' in df.columns:\n",
    "    df['origin_datetime'] = pd.to_datetime(df['origin_ts'], unit='ms')\n",
    "    df['hour'] = df['origin_datetime'].dt.hour\n",
    "\n",
    "heat_data_by_hour = []\n",
    "hour_labels = []\n",
    "\n",
    "# Process data for each hour\n",
    "for hour in range(24):\n",
    "    hour_data = df[df['hour'] == hour]\n",
    "    \n",
    "    hour_heat_data = [[row['origin_stop_lat'], row['origin_stop_lon'], 1] for _, row in hour_data.iterrows()]\n",
    "    heat_data_by_hour.append(hour_heat_data)\n",
    "    \n",
    "    hour_labels.append(f\"{hour}:00\")\n",
    "\n",
    "HeatMapWithTime(\n",
    "    heat_data_by_hour,\n",
    "    index=hour_labels,\n",
    "    auto_play=True,\n",
    "    max_opacity=0.8,\n",
    "    radius=15,\n",
    "    gradient={\n",
    "        0.2: 'blue',\n",
    "        0.4: 'lime',\n",
    "        0.6: 'yellow',\n",
    "        0.8: 'orange',\n",
    "        1.0: 'red'\n",
    "    },\n",
    "    min_opacity=0.5,\n",
    "    use_local_extrema=True  \n",
    ").add_to(m4)\n",
    "\n",
    "title_html = '''\n",
    "<div style=\"position: fixed; top: 10px; left: 50%; transform: translateX(-50%); \n",
    "background-color: white; padding: 10px; border: 2px solid grey; border-radius: 5px; z-index: 1000;\">\n",
    "<h3>Public Transport Usage by Hour (origin locations)</h3>\n",
    "</div>\n",
    "'''\n",
    "m4.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "m4.save('hourly_heatmap.html')\n",
    "display(m4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commuters vs occasional users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment users based on frequency, assuming 'pan' is the user identifier\n",
    "user_freq = df.groupby('pan').size().reset_index(name='journey_count')\n",
    "user_freq['user_type'] = pd.cut(\n",
    "    user_freq['journey_count'], \n",
    "    bins=[0, 2, 4, 10, float('inf')],\n",
    "    labels=['One-time', 'Regular', 'Frequent', 'Very Frequent'],\n",
    "    right=False  # Ensure the bins are inclusive on the left\n",
    ")\n",
    "# Ensure 'user_type' is a categorical column\n",
    "user_freq['user_type'] = user_freq['user_type'].astype('category')\n",
    "\n",
    "for label in user_freq['user_type'].cat.categories:\n",
    "    if label == 'One-time':\n",
    "        print(f\"{label}: Users who traveled only once.\")\n",
    "    elif label == 'Regular':\n",
    "        print(f\"{label}: Users who traveled 2-3 times.\")\n",
    "    elif label == 'Frequent':\n",
    "        print(f\"{label}: Users who traveled 4-9 times.\")\n",
    "    elif label == 'Very Frequent':\n",
    "        print(f\"{label}: Users who traveled more than 10 times.\\n\")\n",
    "\n",
    "# count users in each segment\n",
    "user_segment_counts = user_freq['user_type'].value_counts().sort_index()\n",
    "print(\"User segments based on journey count:\")\n",
    "for segment, count in user_segment_counts.items():\n",
    "    print(f\"{segment}: {count} users ({count/len(user_freq)*100:.1f}%)\")\n",
    "\n",
    "# join user frequency \n",
    "df = df.merge(user_freq[['pan', 'journey_count', 'user_type']], on='pan', how='left')\n",
    "\n",
    "# extract hour and determine if peak hours\n",
    "df['hour'] = df['origin_datetime'].dt.hour\n",
    "df['is_morning_peak'] = df['hour'].between(6, 9)\n",
    "df['is_evening_peak'] = df['hour'].between(16, 19)\n",
    "df['is_peak_hour'] = df['is_morning_peak'] | df['is_evening_peak']\n",
    "\n",
    "# Identify commuter patterns\n",
    "# Commuters: Users who travel back from origin to destination \n",
    "user_commute_ratio = df.groupby('pan').agg(\n",
    "    morning_peak=('is_morning_peak', 'any'),\n",
    "    evening_peak=('is_evening_peak', 'any')\n",
    ").reset_index()\n",
    "\n",
    "user_commute_ratio.head()\n",
    "user_commute_ratio['is_commuter'] = user_commute_ratio['morning_peak'] & user_commute_ratio['evening_peak']\n",
    "print(f\"\\nCommuter ratio: {user_commute_ratio['is_commuter'].mean() * 100:.2f}% of users are commuters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = user_segment_counts.sum()\n",
    "percentages = (user_segment_counts / total) * 100\n",
    "\n",
    "# filter out segments with less than 1%\n",
    "filtered_counts = user_segment_counts[percentages >= 1]\n",
    "\n",
    "plt.figure(figsize=(8, 8), facecolor='#1f2937')\n",
    "explode = [0.05] * len(filtered_counts)\n",
    "\n",
    "def autopct_func(pct):\n",
    "    return '{:.1f}%'.format(pct) if pct >= 1 else ''\n",
    "\n",
    "ax = filtered_counts.plot.pie(\n",
    "    autopct=autopct_func,\n",
    "    startangle=90,\n",
    "    counterclock=False,\n",
    "    explode=explode,\n",
    "    textprops={'color': 'white'},\n",
    "    figsize=(8, 8)\n",
    ")\n",
    "ax.set_facecolor('#1f2937')\n",
    "plt.title('Distribution of Users by Journey Frequency', color='white')\n",
    "plt.ylabel('')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'trip_duration' not in df.columns:\n",
    "    df['trip_duration_min'] = (df['dest_ts'] - df['origin_ts']) / 60_000  # 1 min = 60 000 ms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distance between origin and destination using Haversine formula\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate the Haversine distance between two points in kilometers\"\"\"\n",
    "    R = 6371  # Earth radius in kilometers\n",
    "    \n",
    "    # convert latitude and longitude from degrees to radians\n",
    "    lat1_rad = np.radians(lat1)\n",
    "    lon1_rad = np.radians(lon1)\n",
    "    lat2_rad = np.radians(lat2)\n",
    "    lon2_rad = np.radians(lon2)\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    distance = R * c\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate direct distance between origin and destination\n",
    "df['direct_distance_km'] = df.apply(\n",
    "    lambda row: haversine_distance(\n",
    "        row['origin_stop_lat'], \n",
    "        row['origin_stop_lon'], \n",
    "        row['dest_stop_lat'], \n",
    "        row['dest_stop_lon']\n",
    "    ), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# calculate travel speed (km/h)\n",
    "df['avg_speed_kmh'] = df['direct_distance_km'] / (df['trip_duration_min'] / 60)\n",
    "# N.B the distance is direct, not the actual travel distance\n",
    "\n",
    "# convert transport_type to numeric\n",
    "transport_type_mapping = {\n",
    "    'bus': 0, \n",
    "    'metro': 1, \n",
    "    'trolleybus': 2, \n",
    "    'tram': 3\n",
    "}\n",
    "df['transport_type_code'] = df['transport_type'].map(transport_type_mapping)\n",
    "\n",
    "df['is_transfer'] = df['is_transfer'].astype(int)\n",
    "\n",
    "# subset of features for clustering\n",
    "features = df[[\n",
    "    'direct_distance_km',\n",
    "    'trip_duration_min',\n",
    "    'avg_speed_kmh',\n",
    "    'transport_type_code',\n",
    "    'is_transfer',\n",
    "    'hour_of_day',\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing values in features:\")\n",
    "print(features.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the features\n",
    "For example, without scaling, a 5-minute difference in trip duration (small relative to the ~24 minute range) would outweigh a change from no transfer to transfer (which is the maximum possible change in that feature).\n",
    "\n",
    "After standardization, all features are expressed in the same unit: standard deviations from the mean. This makes them directly comparable and ensures no feature dominates the distance calculations in algorithms like K-means simply because it has larger values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering\n",
    "\n",
    "### Determining the Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method\n",
    "inertia = []\n",
    "silhouette_avg = []\n",
    "k_range = range(2, 10)\n",
    "\n",
    "for k in k_range:\n",
    "    print(f\"Calculating KMeans for k={k}...\")\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(features_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    # Calculate Silhouette Score\n",
    "    labels = kmeans.labels_\n",
    "    silhouette_avg.append(silhouette_score(features_scaled, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_range, inertia, 'bo-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_range, silhouette_avg, 'ro-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for Optimal k')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the **silhouette score plot**, we can see that:\n",
    "\n",
    "The highest silhouette score occurs at k=4 clusters. There's a secondary peak at k=6 clusters. The scores generally decrease after k=6\n",
    "\n",
    "From the **elbow method** plot:\n",
    "\n",
    "There's no sharp \"elbow\" point, but there's a gradual decrease in inertia. The rate of decrease noticeably slows around k=4 to k=5\n",
    "\n",
    "Given these observations, **k=4 appears to be the optimal choice** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k = 4\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "df['kmeans_cluster'] = kmeans.fit_predict(features_scaled)\n",
    "\n",
    "cluster_centers = pd.DataFrame(\n",
    "    scaler.inverse_transform(kmeans.cluster_centers_), \n",
    "    columns=features.columns\n",
    ")\n",
    "print(\"\\nCluster Centers:\")\n",
    "display(cluster_centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **transport_type_code**: This column was originally encoded as 0 (bus), 1 (metro), and 2 (trolleybus), 3 (tram). After clustering, we're seeing decimal values like 0.902816, which indicates that Cluster 0 contains a mix of transport types, leaning slightly toward metro (1). Cluster 3 with 2.684802 suggests it contains predominantly trolleybus trips.\n",
    "\n",
    "2. **is_transfer**: This was originally binary (0 or 1). The value 2.034468e-01 (0.2034) in Cluster 0 means approximately 20% of trips in this cluster involve transfers. The value 2.986500e-14 is effectively zero, meaning Cluster 1 has almost no transfers. Cluster 2 with exactly 1.0 consists entirely of transfers.\n",
    "\n",
    "3. **hour_of_day**: The values (around 9-11) seem reasonable for morning/midday trips.\n",
    "\n",
    "These \"strange\" values occur because cluster centers represent the average of all points in a cluster. \n",
    "\n",
    "### Cluster intrpretation \n",
    "\n",
    "- **Cluster 0**: Long-distance trips (7.3 km), longest duration (22.7 min), fastest speed (20.2 km/h), mix of transport types but mostly metro, 20% transfers, early morning (9.8 hour = 9:48 AM)\n",
    "\n",
    "- **Cluster 1**: Medium-distance trips (2.6 km), medium duration (10 min), medium speed (16.7 km/h), mostly metro, virtually no transfers, mid-morning (10.7 hour)\n",
    "\n",
    "- **Cluster 2**: Medium-distance trips (3.1 km), medium duration (11.1 min), medium-fast speed (17.3 km/h), mostly metro, 100% transfers, late morning (11.3 hour)\n",
    "\n",
    "- **Cluster 3**: Short-distance trips (2 km), shortest duration (9.2 min), slowest speed (14 km/h), predominantly trolley and tram, 36% transfers, mid-morning (11 hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN Clustering\n",
    "DBSCAN is good for spatial data and can find clusters of arbitrary shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For large datasets, we're using a sample for initial parameter tuning\n",
    "sample_size = 5000  # Use a 5000-point sample for initial tuning\n",
    "sample_indices = np.random.choice(len(df), sample_size, replace=False)\n",
    "df_sample = df.iloc[sample_indices].copy()\n",
    "\n",
    "features_cols = [\n",
    "    'direct_distance_km',\n",
    "    'trip_duration_min', \n",
    "    'avg_speed_kmh',\n",
    "    'transport_type_code',\n",
    "    'is_transfer',\n",
    "    'hour_of_day', \n",
    "]\n",
    "\n",
    "features_sample = df_sample[features_cols]\n",
    "scaler = StandardScaler()\n",
    "features_sample_scaled = scaler.fit_transform(features_sample)\n",
    "\n",
    "# For a large dataset, appropriate parameter ranges:\n",
    "# 1. eps: Should be smaller than for small datasets\n",
    "# 2. min_samples: Should be larger to avoid excessive fragmentation\n",
    "\n",
    "eps_values = [0.1, 0.2, 0.3, 0.5, 0.7, 1.0]\n",
    "min_samples_values = [10, 20, 50, 100, 200]\n",
    "\n",
    "print(f\"Testing {len(eps_values) * len(min_samples_values)} parameter combinations on {sample_size} sample points...\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# grid search\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(features_sample_scaled)\n",
    "        \n",
    "        # number of clusters excluding noise points with label -1\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = list(labels).count(-1)\n",
    "        \n",
    "        # skip cases with 1 cluster or all points as noise\n",
    "        if n_clusters < 2 or n_noise == len(labels):\n",
    "            silhouette = np.nan\n",
    "        else:\n",
    "            # filter out noise points for silhouette calculation\n",
    "            mask = labels != -1\n",
    "            try:\n",
    "                silhouette = silhouette_score(features_sample_scaled[mask], labels[mask])\n",
    "            except:\n",
    "                silhouette = np.nan\n",
    "        \n",
    "        results.append({\n",
    "            'eps': eps,\n",
    "            'min_samples': min_samples,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_noise': n_noise,\n",
    "            'noise_ratio': n_noise / len(labels),\n",
    "            'silhouette': silhouette\n",
    "        })\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"Top DBSCAN Parameter Combinations:\")\n",
    "display(results_df.sort_values('silhouette', ascending=False).head(10))\n",
    "\n",
    "heatmap_clusters = results_df.pivot(index='eps', columns='min_samples', values='n_clusters')\n",
    "heatmap_silhouette = results_df.pivot(index='eps', columns='min_samples', values='silhouette')\n",
    "heatmap_noise = results_df.pivot(index='eps', columns='min_samples', values='noise_ratio')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "sns.heatmap(heatmap_clusters, annot=True, cmap='viridis', ax=axes[0], fmt='d')\n",
    "axes[0].set_title('Number of Clusters')\n",
    "axes[0].set_xlabel('min_samples')\n",
    "axes[0].set_ylabel('eps')\n",
    "\n",
    "# Silhouette score\n",
    "sns.heatmap(heatmap_silhouette, annot=True, cmap='RdYlGn', ax=axes[1], fmt='.3f')\n",
    "axes[1].set_title('Silhouette Score')\n",
    "axes[1].set_xlabel('min_samples')\n",
    "axes[1].set_ylabel('eps')\n",
    "\n",
    "# Noise ratio\n",
    "sns.heatmap(heatmap_noise, annot=True, cmap='Reds', ax=axes[2], fmt='.2f')\n",
    "axes[2].set_title('Noise Ratio')\n",
    "axes[2].set_xlabel('min_samples')\n",
    "axes[2].set_ylabel('eps')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter results - not too many/few clusters, not too much noise\n",
    "filtered_results = results_df[\n",
    "    (results_df['n_clusters'] >= 3) & \n",
    "    (results_df['n_clusters'] <= 10) & \n",
    "    (results_df['noise_ratio'] < 0.3) &\n",
    "    (~results_df['silhouette'].isna())\n",
    "]\n",
    "display(filtered_results.sort_values('silhouette', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not filtered_results.empty): \n",
    "        # Sort by silhouette score\n",
    "        filtered_results = filtered_results.sort_values('silhouette', ascending=False).reset_index(drop=True)\n",
    "        best_result = filtered_results.iloc[0]\n",
    "        # Now apply to the full dataset\n",
    "        print(\"\\nApplying optimal parameters to full dataset...\")\n",
    "        \n",
    "        # Prepare the full dataset\n",
    "        features_full = df[features_cols]\n",
    "        features_full = features_full.fillna(features_full.mean())\n",
    "        features_full_scaled = scaler.transform(features_full)\n",
    "        \n",
    "        optimal_dbscan = DBSCAN(\n",
    "            eps=best_result['eps'], \n",
    "            min_samples=int(best_result['min_samples']),\n",
    "            n_jobs=-1  # Use all available cores for faster processing\n",
    "        )\n",
    "        df['dbscan_cluster'] = optimal_dbscan.fit_predict(features_full_scaled)\n",
    "        \n",
    "        n_clusters_full = len(set(df['dbscan_cluster'])) - (1 if -1 in df['dbscan_cluster'] else 0)\n",
    "        n_noise_full = list(df['dbscan_cluster']).count(-1)\n",
    "        \n",
    "        print(f\"Number of clusters in full dataset: {n_clusters_full}\")\n",
    "        print(f\"Noise points in full dataset: {n_noise_full} ({n_noise_full/len(df):.2%})\")\n",
    "        \n",
    "        cluster_sizes = df['dbscan_cluster'].value_counts().sort_index()\n",
    "        print(\"\\nCluster sizes:\")\n",
    "        display(cluster_sizes)\n",
    "        \n",
    "        try:\n",
    "            cluster_stats = df.groupby('dbscan_cluster').agg({\n",
    "                'direct_distance_km': ['mean', 'std'],\n",
    "                'trip_duration_min': ['mean', 'std'],\n",
    "                'avg_speed_kmh': ['mean', 'std'],\n",
    "                'is_transfer': 'mean',\n",
    "                'hour_of_day': 'mean'\n",
    "            })\n",
    "            \n",
    "            print(\"\\nCluster statistics:\")\n",
    "            display(cluster_stats)\n",
    "        except:\n",
    "            print(\"Could not calculate all cluster statistics. Adjust the column names as needed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
